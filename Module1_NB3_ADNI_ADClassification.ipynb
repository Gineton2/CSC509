{"cells":[{"cell_type":"markdown","metadata":{"id":"RTyQ_XD8ctVP"},"source":["## Problem definition: Classification of PET images into healthy and AD.\n","\n","\n","In this jupyter notebook, you will learn to classify PET images into health/congitively normal (CN) and Alzheimer's disease (AD) by using transfer learning using a pre-trained network. \n","\n","## You will learn 2 ways of customizing a pre-trained network:\n","1. Feature extraction: Use the representations learnt by the pre-trained model to extract features from the new images. You just add a classifer layer which will be trained from scratch on top of the pretrained model. You do not need to retrain the entire model.\n","2. Fine-tuning: Unfreeze few top layers of the frozen model and jointly train with the newly added classifier layer and the last layers of the base modelThie allows us to \"fine-tune\" the higher order representations in the base model to make it more suitable for the task in hand. .  \n","\n","\n","## Objectivies (What will we learn here?)\n","\n","1. Setting up the env\n","2. Create training, validation and test ids\n","3. Image pre-processing\n","4. Image augmentation\n","5. Building the classification model\n","6. Feature extraction\n","7. Looking at learning curves\n","8. Fine-tuning\n","9. Looking at the learning curves\n","10. Predicting using the build model\n","11. What next? It is time to play!"]},{"cell_type":"markdown","metadata":{"id":"E2zRi54DdQiH"},"source":["## Step 1: Set up the env"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3741,"status":"ok","timestamp":1638483753184,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"AnxbFfS2dODK","outputId":"011cd458-5cab-404c-ba12-80db62388f5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.7.0\n"]}],"source":["import os\n","import numpy as np\n","from numpy import expand_dims\n","import cv2\n","import nibabel as nib \n","import matplotlib.pyplot as plt\n","import glob\n","import pandas as pd\n","from pathlib import Path\n","from google.colab import drive\n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow import keras\n","from keras import layers\n","import keras.backend as K\n","from keras.models import Model\n","from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D, BatchNormalization, Dropout\n","from tensorflow.keras.layers.experimental import preprocessing \n","from tensorflow.keras.optimizers import *\n","from sklearn.model_selection import train_test_split\n","from IPython.display import clear_output\n","from skimage.color import gray2rgb\n","\n","\n","tf.config.run_functions_eagerly(True)"]},{"cell_type":"markdown","metadata":{"id":"kNLiP_yr_-xa"},"source":["Let's install pydicom which is the library we will use for reading dicom images"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5516,"status":"ok","timestamp":1638483758696,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"NmjlzALzjQo2","outputId":"eae19649-48f7-4c5a-b6c3-28dfb64a7115"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pydicom\n","  Downloading pydicom-2.2.2-py3-none-any.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 10.9 MB/s \n","\u001b[?25hInstalling collected packages: pydicom\n","Successfully installed pydicom-2.2.2\n"]}],"source":["!pip install pydicom"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1638483758697,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"XVuhGwZTjUWa"},"outputs":[],"source":["import pydicom"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1638483758697,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"yxLO7yz5doEp"},"outputs":[],"source":["from google.colab import drive"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31967,"status":"ok","timestamp":1638483790661,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"7L6M2WlCgAgr","outputId":"55b1efc3-7eef-47cf-abaf-b26978595915"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1638483790661,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"Na6-KOYjd6dl"},"outputs":[],"source":["DATA_PATH = Path('/content/drive/MyDrive/Data_Shortcut/Module1_ADNI/')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30501,"status":"ok","timestamp":1638483821158,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"zgYtGoDrgOu0","outputId":"7093b477-1f92-4445-9635-fc0f4fa9efdb"},"outputs":[{"data":{"text/plain":["['ADNI_PET_MiddleSlices_UniformResolution_Averages.csv',\n"," 'ADNI_2_AV45_base_10_01_2021.csv',\n"," 'ADNI2_3_base_screen_AV45_10_08_2021.csv',\n"," 'adnim.csv',\n"," 'ADNI_Amyloid_Status_AV45_KAIST.csv',\n"," 'ADNI_Amyloid_Status_UPenn_UCBerkeley.csv',\n"," 'ADNI_PET_MiddleSlices_ClinicalData.csv']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir(Path(DATA_PATH, 'CSVS',))"]},{"cell_type":"markdown","metadata":{"id":"tYFysOI4ARrS"},"source":["From the data wrangling and preprocessing exercise we have already created a csv with middle slices of the PET images. Let's reat that csv file. "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":540,"status":"ok","timestamp":1638483821685,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"BKM3ARWmgpWq","outputId":"d2aa52dd-2ca3-464a-d83e-abdf551e6b5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["392\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003eFILEPATH_MIDDLE_SLICES\u003c/th\u003e\n","      \u003cth\u003eAVG_OR_DYN\u003c/th\u003e\n","      \u003cth\u003eSLICE_NUM\u003c/th\u003e\n","      \u003cth\u003ePTID\u003c/th\u003e\n","      \u003cth\u003eImage Data ID\u003c/th\u003e\n","      \u003cth\u003eSubject\u003c/th\u003e\n","      \u003cth\u003eGroup\u003c/th\u003e\n","      \u003cth\u003eSex\u003c/th\u003e\n","      \u003cth\u003eAge\u003c/th\u003e\n","      \u003cth\u003eVisit\u003c/th\u003e\n","      \u003cth\u003eModality\u003c/th\u003e\n","      \u003cth\u003eDescription\u003c/th\u003e\n","      \u003cth\u003eType\u003c/th\u003e\n","      \u003cth\u003eAcq Date\u003c/th\u003e\n","      \u003cth\u003eFormat\u003c/th\u003e\n","      \u003cth\u003eDownloaded\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_0295\u003c/td\u003e\n","      \u003ctd\u003eI240520\u003c/td\u003e\n","      \u003ctd\u003e002_S_0295\u003c/td\u003e\n","      \u003ctd\u003eCN\u003c/td\u003e\n","      \u003ctd\u003eM\u003c/td\u003e\n","      \u003ctd\u003e90\u003c/td\u003e\n","      \u003ctd\u003e26\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Coreg, Avg, Standardized Image and Voxel ...\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e6/10/2011\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/16/2021\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_1155\u003c/td\u003e\n","      \u003ctd\u003eI856316\u003c/td\u003e\n","      \u003ctd\u003e002_S_1155\u003c/td\u003e\n","      \u003ctd\u003eMCI\u003c/td\u003e\n","      \u003ctd\u003eM\u003c/td\u003e\n","      \u003ctd\u003e68\u003c/td\u003e\n","      \u003ctd\u003e101\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Coreg, Avg, Std Img and Vox Siz, Uniform ...\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e4/20/2017\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/03/2021\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_1280\u003c/td\u003e\n","      \u003ctd\u003eI856333\u003c/td\u003e\n","      \u003ctd\u003e002_S_1280\u003c/td\u003e\n","      \u003ctd\u003eCN\u003c/td\u003e\n","      \u003ctd\u003eF\u003c/td\u003e\n","      \u003ctd\u003e81\u003c/td\u003e\n","      \u003ctd\u003e101\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Coreg, Avg, Standardized Image and Voxel ...\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e3/02/2017\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/01/2021\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_4213\u003c/td\u003e\n","      \u003ctd\u003eI259148\u003c/td\u003e\n","      \u003ctd\u003e002_S_4213\u003c/td\u003e\n","      \u003ctd\u003eCN\u003c/td\u003e\n","      \u003ctd\u003eF\u003c/td\u003e\n","      \u003ctd\u003e78\u003c/td\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Co-registered Dynamic\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e9/23/2011\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/16/2021\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_4251\u003c/td\u003e\n","      \u003ctd\u003eI274142\u003c/td\u003e\n","      \u003ctd\u003e002_S_4251\u003c/td\u003e\n","      \u003ctd\u003eLMCI\u003c/td\u003e\n","      \u003ctd\u003eM\u003c/td\u003e\n","      \u003ctd\u003e72\u003c/td\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Coreg, Avg, Standardized Image and Voxel ...\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e12/19/2011\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/16/2021\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["   Unnamed: 0  ... Downloaded\n","0           0  ...  9/16/2021\n","1           1  ...  9/03/2021\n","2           2  ...  9/01/2021\n","3           3  ...  9/16/2021\n","4           4  ...  9/16/2021\n","\n","[5 rows x 17 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data_df = pd.read_csv(Path(DATA_PATH,'CSVS/ADNI_PET_MiddleSlices_ClinicalData.csv'))\n","data_df = data_df[data_df['SLICE_NUM'] == 48].reset_index(drop = True)\n","print(len(data_df))\n","data_df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1638483821686,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"z0qpkSlruLm_"},"outputs":[],"source":["data_df['FOLDERPATH_MIDDLE_SLICES'] = data_df['FILEPATH_MIDDLE_SLICES'].str.rsplit('/', n = 1).str[0]"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"elapsed":1148,"status":"ok","timestamp":1638483822829,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"0yyO38ehZbWd","outputId":"da1e5c6f-f48b-4213-9d91-6f0dd9bbc3da"},"outputs":[{"name":"stdout","output_type":"stream","text":["1238\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePTID\u003c/th\u003e\n","      \u003cth\u003eGroup_AB\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e011_S_0003\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e022_S_0004\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e011_S_0005\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e011_S_0008\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e011_S_0010\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["         PTID  Group_AB\n","0  011_S_0003       1.0\n","1  022_S_0004       0.0\n","2  011_S_0005       1.0\n","3  011_S_0008       0.0\n","4  011_S_0010       1.0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["data_status = pd.read_csv(Path(DATA_PATH,'CSVS/ADNI_Amyloid_Status_UPenn_UCBerkeley.csv'))\n","\n","data_status = data_status[~(data_status['upenn_pos_bl'].isnull())]\n","data_status = data_status[['PTID', 'upenn_pos_bl']].drop_duplicates().reset_index(drop = True)\n","data_status = data_status.rename(columns = {\"upenn_pos_bl\": \"Group_AB\"})\n","print(len(data_status))\n","\n","\n","data_status.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":574},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1638483823413,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"mI4CcXvjZ8c_","outputId":"689def52-b755-4768-a0a1-72e74e21b050"},"outputs":[{"name":"stdout","output_type":"stream","text":["347\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eUnnamed: 0\u003c/th\u003e\n","      \u003cth\u003eFILEPATH_MIDDLE_SLICES\u003c/th\u003e\n","      \u003cth\u003eAVG_OR_DYN\u003c/th\u003e\n","      \u003cth\u003eSLICE_NUM\u003c/th\u003e\n","      \u003cth\u003ePTID\u003c/th\u003e\n","      \u003cth\u003eImage Data ID\u003c/th\u003e\n","      \u003cth\u003eSubject\u003c/th\u003e\n","      \u003cth\u003eGroup\u003c/th\u003e\n","      \u003cth\u003eSex\u003c/th\u003e\n","      \u003cth\u003eAge\u003c/th\u003e\n","      \u003cth\u003eVisit\u003c/th\u003e\n","      \u003cth\u003eModality\u003c/th\u003e\n","      \u003cth\u003eDescription\u003c/th\u003e\n","      \u003cth\u003eType\u003c/th\u003e\n","      \u003cth\u003eAcq Date\u003c/th\u003e\n","      \u003cth\u003eFormat\u003c/th\u003e\n","      \u003cth\u003eDownloaded\u003c/th\u003e\n","      \u003cth\u003eFOLDERPATH_MIDDLE_SLICES\u003c/th\u003e\n","      \u003cth\u003eGroup_AB\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_0295\u003c/td\u003e\n","      \u003ctd\u003eI240520\u003c/td\u003e\n","      \u003ctd\u003e002_S_0295\u003c/td\u003e\n","      \u003ctd\u003eCN\u003c/td\u003e\n","      \u003ctd\u003eM\u003c/td\u003e\n","      \u003ctd\u003e90\u003c/td\u003e\n","      \u003ctd\u003e26\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Coreg, Avg, Standardized Image and Voxel ...\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e6/10/2011\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/16/2021\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_4213\u003c/td\u003e\n","      \u003ctd\u003eI259148\u003c/td\u003e\n","      \u003ctd\u003e002_S_4213\u003c/td\u003e\n","      \u003ctd\u003eCN\u003c/td\u003e\n","      \u003ctd\u003eF\u003c/td\u003e\n","      \u003ctd\u003e78\u003c/td\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Co-registered Dynamic\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e9/23/2011\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/16/2021\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e4\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_4251\u003c/td\u003e\n","      \u003ctd\u003eI274142\u003c/td\u003e\n","      \u003ctd\u003e002_S_4251\u003c/td\u003e\n","      \u003ctd\u003eLMCI\u003c/td\u003e\n","      \u003ctd\u003eM\u003c/td\u003e\n","      \u003ctd\u003e72\u003c/td\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Coreg, Avg, Standardized Image and Voxel ...\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e12/19/2011\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/16/2021\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e5\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_4447\u003c/td\u003e\n","      \u003ctd\u003eI285198\u003c/td\u003e\n","      \u003ctd\u003e002_S_4447\u003c/td\u003e\n","      \u003ctd\u003eEMCI\u003c/td\u003e\n","      \u003ctd\u003eF\u003c/td\u003e\n","      \u003ctd\u003e68\u003c/td\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Coreg, Avg, Std Img and Vox Siz, Uniform ...\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e2/13/2012\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/16/2021\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e7\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003eAvg\u003c/td\u003e\n","      \u003ctd\u003e48\u003c/td\u003e\n","      \u003ctd\u003e002_S_4746\u003c/td\u003e\n","      \u003ctd\u003eI311757\u003c/td\u003e\n","      \u003ctd\u003e002_S_4746\u003c/td\u003e\n","      \u003ctd\u003eLMCI\u003c/td\u003e\n","      \u003ctd\u003eF\u003c/td\u003e\n","      \u003ctd\u003e71\u003c/td\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","      \u003ctd\u003ePET\u003c/td\u003e\n","      \u003ctd\u003eAV45 Coreg, Avg, Standardized Image and Voxel ...\u003c/td\u003e\n","      \u003ctd\u003eProcessed\u003c/td\u003e\n","      \u003ctd\u003e6/14/2012\u003c/td\u003e\n","      \u003ctd\u003eDCM\u003c/td\u003e\n","      \u003ctd\u003e9/16/2021\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["   Unnamed: 0  ... Group_AB\n","0           0  ...      1.0\n","1           3  ...      0.0\n","2           4  ...      1.0\n","3           5  ...      1.0\n","4           7  ...      1.0\n","\n","[5 rows x 19 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Merge the 2 dataframes\n","df_merge = pd.merge(data_df, data_status, on = \"PTID\", how = 'inner')\n","print(len(df_merge))\n","data_df = df_merge\n","data_df.head()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1638483823413,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"jTgFA7d6jljr","outputId":"5feedb71-2e29-485e-f828-999a0e01fedd"},"outputs":[{"data":{"text/plain":["array([1., 0.])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Let's check the groups\n","#data_df['Group'].unique()\n","data_df['Group_AB'].unique()"]},{"cell_type":"markdown","metadata":{"id":"sjlDJ9-CAf33"},"source":["## We already understand what each of these groups mean:\n","\n","- CN : Cognitively normal\n","- MCI : Mild cognitive impairment\n","- EMCI : Early mild cognitive impairment\n","- LMCI: Late mild cognitive impairment\n","- AD : Alzheimer's disease\n","- SMC : Significant memory concerns"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1638483823414,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"98HjynRyg_yP","outputId":"13a05872-de62-49b1-fb5a-480cffaf8c96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of patient ids: 347\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePTID\u003c/th\u003e\n","      \u003cth\u003eFOLDERPATH_MIDDLE_SLICES\u003c/th\u003e\n","      \u003cth\u003eGroup_AB\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e002_S_0295\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e002_S_4213\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e0.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e002_S_4251\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e002_S_4447\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e002_S_4746\u003c/td\u003e\n","      \u003ctd\u003e/content/drive/MyDrive/Data_Shortcut/Module1_A...\u003c/td\u003e\n","      \u003ctd\u003e1.0\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["         PTID                           FOLDERPATH_MIDDLE_SLICES  Group_AB\n","0  002_S_0295  /content/drive/MyDrive/Data_Shortcut/Module1_A...       1.0\n","1  002_S_4213  /content/drive/MyDrive/Data_Shortcut/Module1_A...       0.0\n","2  002_S_4251  /content/drive/MyDrive/Data_Shortcut/Module1_A...       1.0\n","3  002_S_4447  /content/drive/MyDrive/Data_Shortcut/Module1_A...       1.0\n","4  002_S_4746  /content/drive/MyDrive/Data_Shortcut/Module1_A...       1.0"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# We will train a binary classifier for CN v/s AD, so let's filter for these values of the column Group.\n","#data_df = data_df[['Image Data ID', 'FILEPATH_MIDDLE_SLICES', 'Group' ]]\n","#data_df_filt = data_df[data_df['Group'].isin(['AD', 'CN'])].reset_index(drop = True)\n","data_df_filt = data_df[['PTID', 'FOLDERPATH_MIDDLE_SLICES', 'Group_AB' ]]\n","\n","print(\"Number of patient ids:\", len(data_df_filt))\n","data_df_filt.head()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1638483823414,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"EOL5HnoDoje7","outputId":"70333ced-178e-48b2-cd1d-acd0faf6185f"},"outputs":[{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePTID\u003c/th\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eGroup_AB\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0.0\u003c/th\u003e\n","      \u003ctd\u003e148\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1.0\u003c/th\u003e\n","      \u003ctd\u003e199\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["          PTID\n","Group_AB      \n","0.0        148\n","1.0        199"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Let's check the number of patients with CN and AD\n","#data_df_filt[['Image Data ID', 'Group']].groupby(by = 'Group').count()\n","data_df_filt[['PTID', 'Group_AB']].groupby(by = 'Group_AB').count()"]},{"cell_type":"markdown","metadata":{"id":"Obcmp67iBpEI"},"source":["## Step 2: Now let's create the training, validation and test ids."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1638483823414,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"PnI7rMHmmbjC","outputId":"1788c407-792a-4c79-de41-92e476f930d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 249 patient ids in training set\n","There are 45 patient ids in validation set\n","There are 53 patient ids in test set\n"]}],"source":["# Split into training, validation set and test set\n","#ids = data_df_filt['Image Data ID'].unique()\n","ids = data_df_filt['PTID'].unique()\n","\n","# Here we have selected the size of test set as 15 %\n","train_ids, test_ids = train_test_split(ids,test_size=0.15) \n","\n","# Create validation ids by further splitting the train ids, we again use 15 % as size of validation set. Validation set is also referred to as tuning set. \n","train_ids, val_ids = train_test_split(train_ids,test_size=0.15) \n","\n","print(f'There are {len(train_ids)} patient ids in training set')\n","print(f'There are {len(val_ids)} patient ids in validation set')\n","print(f'There are {len(test_ids)} patient ids in test set')\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":601},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1638483823415,"user":{"displayName":"Neha Anegondi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg__zLIlbkTqXaUfZcjnWMEOQmbatMFjynTasRbjQ=s64","userId":"13446886544877956139"},"user_tz":480},"id":"2ThJvB0DrvjH","outputId":"ae2ca3c7-6d98-4c6d-89e9-a33b3d0cd593"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # Remove the CWD from sys.path while we load stuff.\n"]},{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ePTID\u003c/th\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003eSplit\u003c/th\u003e\n","      \u003cth\u003eGroup_AB\u003c/th\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth rowspan=\"2\" valign=\"top\"\u003eTest\u003c/th\u003e\n","      \u003cth\u003e0.0\u003c/th\u003e\n","      \u003ctd\u003e23\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1.0\u003c/th\u003e\n","      \u003ctd\u003e30\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth rowspan=\"2\" valign=\"top\"\u003eTrain\u003c/th\u003e\n","      \u003cth\u003e0.0\u003c/th\u003e\n","      \u003ctd\u003e105\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1.0\u003c/th\u003e\n","      \u003ctd\u003e144\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth rowspan=\"2\" valign=\"top\"\u003eValidation\u003c/th\u003e\n","      \u003cth\u003e0.0\u003c/th\u003e\n","      \u003ctd\u003e20\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1.0\u003c/th\u003e\n","      \u003ctd\u003e25\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e"],"text/plain":["                     PTID\n","Split      Group_AB      \n","Test       0.0         23\n","           1.0         30\n","Train      0.0        105\n","           1.0        144\n","Validation 0.0         20\n","           1.0         25"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Let's check the number of CN and AD patients in the splits. We want our splits to be balanced.\n","#data_df_filt['Split'] = np.where(data_df_filt['Image Data ID'].isin(train_ids), 'Train', '')\n","#data_df_filt['Split'] = np.where(data_df_filt['Image Data ID'].isin(val_ids), 'Validation', data_df_filt['Split'])\n","#data_df_filt['Split'] = np.where(data_df_filt['Image Data ID'].isin(test_ids), 'Test', data_df_filt['Split'])\n","\n","#data_df_filt[['Image Data ID', 'Group', 'Split']].groupby(by = ['Split', 'Group']).count()\n","\n","data_df_filt['Split'] = np.where(data_df_filt['PTID'].isin(train_ids), 'Train', '')\n","data_df_filt['Split'] = np.where(data_df_filt['PTID'].isin(val_ids), 'Validation', data_df_filt['Split'])\n","data_df_filt['Split'] = np.where(data_df_filt['PTID'].isin(test_ids), 'Test', data_df_filt['Split'])\n","\n","data_df_filt[['PTID', 'Group_AB', 'Split']].groupby(by = ['Split', 'Group_AB']).count()"]},{"cell_type":"markdown","metadata":{"id":"xrBCo4RYHBGh"},"source":["## Step 3: Let's do some image pre-processing. You have already learnt basics of reading and visualizing dicom images, selecting slices and looking at histograms.\n","\n","Here, we will look at some pre-processing steps specific to classification problem. It's important to understand that the pro-processing steps could vary depending on the task at hand."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0cR9mErh7gKi"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in short_scalars\n","  # Remove the CWD from sys.path while we load stuff.\n"]},{"name":"stdout","output_type":"stream","text":["56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n"]}],"source":["# Let's create a function to normalize the image between -1 and 1 and change label to 0 and 1\n","def normalize(image, label):\n","    \"\"\"\n","    Will read the input image and normalize the image between -1 and 1.\n","    \"\"\"\n","    #input_image = tf.cast(image, tf.float32) / 32767.0\n","    \n","    img_min = np.min(image)\n","    img_max = np.max(image)\n","    img = (image - img_min) / (img_max - img_min)\n","    input_image = tf.cast(img, tf.float32)\n","    input_label = 0 if label == 0.0 else 1\n","\n","\n","    return input_image, input_label\n","\n","# read dicoms and sort by slice number\n","def read_all_dicom_slices(dicom_directory):\n","    \"\"\"\n","    Will read in the images from a single dicom folder and return a dictionary of \n","    those images with the key as the presumed slice number, and the value being the pixel array. \n","    \n","    dicom_directory: Path or string that leads to the folder containing the slices \n","\n","    \"\"\"\n","    dicom_image_dict = {}\n","    for image in glob.glob(str(Path(dicom_directory, '*'))): \n","      key = image.split('/')[-1].split('_')[-3]\n","      dicom_image_dict[key]= pydicom.dcmread(image)\n","    return dicom_image_dict\n","\n","# Let's create numpy arrays of training, validation and test images.\n","\n","# For the sake of simplicity, we will use only the middle slice of each volume for this experiment.\n","'''\n","def create_numpy_array(df):\n","    X = []\n","    Y = []\n","  \n","    for i in range(len(df)):\n","        \n","        ds = pydicom.dcmread(df['FILEPATH_MIDDLE_SLICES'][i])\n","        image = gray2rgb(ds.pixel_array)\n","        #label = df['Group'][i]  \n","        label = df['Group_AB'][i]\n","        new_image, new_label = normalize(image, label)  \n","        X.append(new_image)\n","        Y.append(new_label)   \n","          \n","    return np.asarray(X), np.asarray(Y)\n","'''\n","def create_numpy_array(df):\n","  X = []\n","  Y = []\n","  for i in range(len(df)):\n","    print(i)\n","    dicom_volume = read_all_dicom_slices(df['FOLDERPATH_MIDDLE_SLICES'][i])\n","    keys_sorted = [str(x) for x in sorted([int(x) for x in list(dicom_volume.keys())])]\n","    keys_filt = [47, 48, 49]\n","    for key in keys_filt: # iterates through the dictionary, while the enumerate function keeps track of the index with j\n","        value = dicom_volume[str(key)]\n","        image = value.pixel_array\n","        label = df['Group_AB'][i]\n","        new_image, new_label = normalize(image, label)  \n","        X.append(new_image)\n","        Y.append(new_label) \n","\n","X_train, Y_train = create_numpy_array(data_df_filt[data_df_filt['Split'] == 'Train'].reset_index(drop = True))\n","X_val, Y_val = create_numpy_array(data_df_filt[data_df_filt['Split'] == 'Validation'].reset_index(drop = True))\n","X_test, Y_test = create_numpy_array(data_df_filt[data_df_filt['Split'] == 'Test'].reset_index(drop = True))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjrObAUaFuQ3"},"outputs":[],"source":["# Let's shuffle the train, validation and test arrays\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_c58jRYPITVE"},"outputs":[],"source":["# Let's confirm the size of the training and validation arrays\n","print(f'There are {len(X_train)} images in training set')\n","print(f'There are {len(X_val)} images in validation set')\n","print(f'There are {len(X_test)} images in test set')\n","\n","\n","# Visualize the images (no AD patients images right now so both images are normal - To change)\n","plt.figure(figsize=(10,20))\n","plt.subplot(1,2,1)\n","plt.imshow(X_train[1][:,:,0], cmap = 'jet')\n","plt.axis('off')\n","plt.title('AD', fontsize=20)\n","plt.subplot(1,2,2)\n","plt.imshow(X_train[5][:,:,0], cmap = 'jet')\n","plt.axis('off')\n","plt.title('CN', fontsize=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVBMmFCuFtFb"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpAgGg8w30QP"},"outputs":[],"source":["# Let's look at the PET images of CN and AD patients\n","plt.figure(figsize = (20, 100))\n","for i in range(100):\n","    \n","    plt.subplot(20,5, i+1) \n","    plt.imshow(X_train[i][:,:,0], cmap = 'jet')\n","    plt.axis('off')\n","    plt.title(Y_train[i])\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nh-Tp8jw_d6q"},"outputs":[],"source":["# Let's create the train, validation and test datasets\n","dataset_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n","dataset_val = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n","dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZdFwkfjGKrJ"},"outputs":[],"source":["BATCH_SIZE = 8\n","SHUFFLE_BUFFER_SIZE = 100\n","\n","train_dataset = dataset_train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n","val_dataset = dataset_val.batch(BATCH_SIZE)\n","test_dataset = dataset_test.batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLgjG6PiGt_i"},"outputs":[],"source":["val_batches = tf.data.experimental.cardinality(val_dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4uRBuUOaG2up"},"outputs":[],"source":["print('Number of validation batches: %d' % tf.data.experimental.cardinality(val_dataset))\n","print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hsgIVSlG-_v"},"outputs":[],"source":["# Configure the data for performance\n","# Use buffered prefetching to load images from disk without having I/O become blocking.\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n","val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n","test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"Lmxl3FoncRNt"},"source":["## Step 4: Let's look at image augmentation now.\n","\n","Augmentation is a technique to increase the diversity of your training set by applying random (but realistic) transformations, such as image rotation. This helps expose the model to different aspects of the training data and reduce overfitting.\n","\n","Note: Augmentation should be applied only on the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfHDKDpbHI2-"},"outputs":[],"source":["# Let's look at examples of image augmentation now\n","\n","# We will create a few pre-processing layers and apply it to the PET image to understand how augmentation works\n","\n","data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n","  tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n","])\n","\n","for image, _ in train_dataset.take(1):\n","  plt.figure(figsize=(10, 10))\n","  first_image = image[0]\n","\n","  print(first_image.shape)\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    augmented_image = data_augmentation(tf.expand_dims(first_image,0))\n","    plt.imshow(augmented_image[0], cmap = 'jet')\n","    plt.axis('off')\n"]},{"cell_type":"markdown","metadata":{"id":"nCbnGl87crE6"},"source":["## Step 5: Let's build our model now. For the first run, we will use MobileNetV2 pre-trained network.This model expects pixel values in [-1, 1].\n","\n","A pre-trained network has already learnt to extract powerful and informative features from natural images and we will use it as a starting point to learn a new task.MobileNetV2 is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like jackfruit and syringe. This base of knowledge will help  classify CN and AD from this dataset.\n","\n","First, we will need to pick which layer of MobileNet V2 we will use for feature extraction. The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, we will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n","\n","First, we will instantiate a MobileNetV2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5RS2CORHdNn"},"outputs":[],"source":["# Create the base model from the pre-trained model MobileNet V2\n","IMG_SIZE = (160, 160)\n","IMG_SHAPE = IMG_SIZE + (3,)\n","base_model = tf.keras.applications.VGG19(input_shape=IMG_SHAPE,\n","                                               include_top=False,\n","                                               weights='imagenet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzyV1_ijetzp"},"outputs":[],"source":["# Let's take a look at the base model architecture\n","base_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"j2EYVXe4egbT"},"source":["This feature extractor converts each 160x160x3 image into a 5x5x1280 block of features. Let's see what it does to an example batch of images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIPBQbInHprI"},"outputs":[],"source":["image_batch, label_batch = next(iter(train_dataset))\n","feature_batch = base_model(image_batch)\n","print(feature_batch.shape)"]},{"cell_type":"markdown","metadata":{"id":"BHvyiahCe3yF"},"source":["## Step 6: Feature extraction\n","\n","\n","In this step, we will freeze the convolutional base created from the previous step and use as a feature extractor. Additionally, we will add a classifier on top of it and train the top-level classifier.\n","\n","It is important to freeze the convolutional base before we compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbtLfsORe3U6"},"outputs":[],"source":["# MobileNetV2 has many layers, so setting the entire model's trainable flag to False will freeze all of them.\n","base_model.trainable = False\n"]},{"cell_type":"markdown","metadata":{"id":"sVAXLydzfkLI"},"source":["Many models contain tf.keras.layers.BatchNormalization layers. This layer is a special case and precautions should be taken in the context of fine-tuning, as shown later in this notebook.\n","\n","When we set layer.trainable = False, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics.\n","\n","When we unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, we should keep the BatchNormalization layers in inference mode by passing training = False when calling the base model. Otherwise, the updates applied to the non-trainable weights will destroy what the model has learned."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSYa0uDNJLcD"},"outputs":[],"source":["# Add a classification head. \n","\n","global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","feature_batch_average = global_average_layer(feature_batch)\n","print(feature_batch_average.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z0j0lXAxgW-j"},"source":["We will now apply a tf.keras.layers.Dense layer to convert these features into a single prediction per image.\n","We don't need an activation function here because this prediction will be treated as a logit, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wt_zuneaJknQ"},"outputs":[],"source":["prediction_layer = tf.keras.layers.Dense(1)\n","prediction_batch = prediction_layer(feature_batch_average)\n","print(prediction_batch.shape)"]},{"cell_type":"markdown","metadata":{"id":"uFlMfM1Cgg8R"},"source":["Now let's build a model by using everything we have learnt so far -  the data augmentation, base_model and feature extractor layers using the Keras Functional API. As previously mentioned, use training=False as our model contains a BatchNormalization layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPazqwYSJqU4"},"outputs":[],"source":["preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n","\n","inputs = tf.keras.Input(shape=(160, 160, 3))\n","x = data_augmentation(inputs)\n","x = preprocess_input(x)\n","x = base_model(x, training=False)\n","x = global_average_layer(x)\n","x = tf.keras.layers.Dropout(0.2)(x)\n","outputs = prediction_layer(x)\n","model = tf.keras.Model(inputs, outputs)"]},{"cell_type":"markdown","metadata":{"id":"tlPNdP3HgvLA"},"source":["Let's compile the model before training it. Since there are two classes, we will use the tf.keras.losses.BinaryCrossentropy loss with from_logits=True since the model provides a linear output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtmSk41YJwPz"},"outputs":[],"source":["# Compile the model\n","\n","base_learning_rate = 0.0001\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1wznIz3J3KI"},"outputs":[],"source":["# Let's look at the model summary\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"gxJnx6xJhDhW"},"source":["The 2.5M parameters in MobileNetV2 are frozen, but there are 1.2K trainable parameters in the Dense layer. These are divided between two tf.Variable objects, the weights and biases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ass6IoeyJ4JU"},"outputs":[],"source":["len(model.trainable_variables)"]},{"cell_type":"markdown","metadata":{"id":"Htuh3veahOn4"},"source":["Let's do the model training now! We will train for 10 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPL2Be1ZJ6uj"},"outputs":[],"source":["# Train the model\n","\n","initial_epochs = 10\n","\n","loss0, accuracy0 = model.evaluate(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyB7xo7qKGFG"},"outputs":[],"source":["print(\"initial loss: {:.2f}\".format(loss0))\n","print(\"initial accuracy: {:.2f}\".format(accuracy0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZHXw_xzKHlm"},"outputs":[],"source":["history = model.fit(train_dataset,\n","                    epochs=initial_epochs,\n","                    validation_data=val_dataset)"]},{"cell_type":"markdown","metadata":{"id":"8EGoKdw4hoe4"},"source":["## Step 7: Now let's take a look at the learning curve.\n","\n","Learning curves are a widely used diagnostic tool in machine learning for algorithms that learn from a training dataset incrementally. The model can be evaluated on the training dataset and on a hold out validation dataset after each update during training and plots of the measured performance can created to show learning curves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XscN8phKT9q"},"outputs":[],"source":["# Learning curves\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hLZTGiXNhzLv"},"source":["## Step 8: Fine-tuning\n","\n","In the feature extraction experiment, were were only training a few layers on top of an MobileNetV2 base model. The weights of the pre-trained network were not updated during training.\n","\n","One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier we added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.\n","\n","Also, it's good to note that we should try to fine-tune a small number of top layers rather than the whole MobileNetV2 model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HoeOCYQvKat_"},"outputs":[],"source":["# Un-freeze the top layers of the model\n","base_model.trainable = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCF0PvlUKhAF"},"outputs":[],"source":["# Let's take a look to see how many layers are in the base model\n","print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","# Fine-tune from this layer onwards\n","fine_tune_at = 8\n","\n","# Freeze all the layers before the `fine_tune_at` layer\n","for layer in base_model.layers[:fine_tune_at]:\n","  layer.trainable =  False"]},{"cell_type":"markdown","metadata":{"id":"pRdxnfUYiRdw"},"source":["Now let's compile the model. As we are training a much larger model and want to readapt the pre-trained weights, it is important to use a lower learning rate at this stage. Otherwise, our model could overfit very quickly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUCyyZ7CKj2s"},"outputs":[],"source":["model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGyXtlhFKnCO"},"outputs":[],"source":["# Let's take a look at the model summary\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UauHjinGKujY"},"outputs":[],"source":["# Let's look at the number of trainable variables now.\n","len(model.trainable_variables)"]},{"cell_type":"markdown","metadata":{"id":"Ueu8qN2BisIA"},"source":["Let's continue to train the model. If we were able to train the model to convergence earlier, this step will further improve the accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBDeUj-bKv_c"},"outputs":[],"source":["fine_tune_epochs = 300\n","total_epochs =  initial_epochs + fine_tune_epochs\n","\n","history_fine = model.fit(train_dataset,\n","                         epochs=total_epochs,\n","                         initial_epoch=history.epoch[-1],\n","                         validation_data=val_dataset)"]},{"cell_type":"markdown","metadata":{"id":"G_5eRFvZi5S4"},"source":["## Step 9: Let's take a look at the learning curves of the training and validation accuracy/loss when fine-tuning the last few layers of the MobileNetV2 base model and training the classifier on top of it. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulNbIMVHK01E"},"outputs":[],"source":["acc += history_fine.history['accuracy']\n","val_acc += history_fine.history['val_accuracy']\n","\n","loss += history_fine.history['loss']\n","val_loss += history_fine.history['val_loss']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Q0gTMF8LFuP"},"outputs":[],"source":["plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.ylim([0.4, 1])\n","plt.plot([initial_epochs-1,initial_epochs-1],\n","          plt.ylim(), label='Start Fine Tuning')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.ylim([0, 1.0])\n","plt.plot([initial_epochs-1,initial_epochs-1],\n","         plt.ylim(), label='Start Fine Tuning')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QWgQLzngjLGT"},"source":["## Step 10: Evaluate the model.\n","\n","We can now verify the performance of the model on new data using test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gWIQHHhLMsX"},"outputs":[],"source":["loss, accuracy = model.evaluate(test_dataset)\n","print('Test accuracy :', accuracy)"]},{"cell_type":"markdown","metadata":{"id":"I-7HjACQjXB5"},"source":["Step 11: What next? Time to play!\n","As a home-work or self learning task, you can try a variety of experiments to understand how the model performance is affected.\n","\n","You can also do some of this as your term project.\n","\n","Try to think about what could improve the model performance?\n","\n","1. Will increasing the size of training set help? We are just using 1 slice per volume, will using more slices help?\n","2. Will changing the hyper-parameters like optimizer, learning rate, loss function, epochs, batch size improve the model performance? For ex. What will happen if you change the optimizer to Adam?\n","3. Will changing the model achitecture help? Check tf.keras.applications for different model architectures like - VGG16, VGG19, InceptionV3, ResNet50, etc.\n","4. How is the learning curve for this trained model? Is it underfitting? Does it need more training? How much training is enough and when should I stop?\n","\n","And there is so much more! This is just a starting point. Model training and hyper-parameter tuning is the fun part of any deep learning project.\n","\n","Next what? In this example we learnt a binary or 2 class classification problem. It can be extended to a multi-class problem.\n","\n","Another fun task to do would be to try a multi-class classification where you can try and predict the different groups like CN, AD, LMCI, etc. What will you need to modify in the current code to change it to a multi-class problem?\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Module1_NB3_ADNI_ADClassification.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}