{"cells":[{"cell_type":"markdown","metadata":{"id":"RTyQ_XD8ctVP"},"source":["# Module 0, Notebook 2: Simple classification of Brain images into MRI vs PET\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/julclu/CSC509/blob/main/Module0/Module0_NB2_v0.ipynb)\n","\n","\n","In this Jupyter notebook, you will learn to build a basic classification model using convolutional neural networks to classify  medical images generated from two different modalities namely PET and MRI. We will be using the Keras API for the classification.\n","\n","In order to enable ease of access we have converted all the DICOM formatted images into JPEG format and randomly sorted them into training and test set. \n","\n","## Some basics on classification problems\n","1. Classification problems can be set-up as a binary (eg. cat vs dogs) or multi-class (eg. cat, dog, chicken and turtle) classification. \n","2. In the current scenario we are doing a **Binary** classification as to whether an image is a MRI image or PET image. \n","3. In classification problems you should pay attention to the performance metrics such as Accuracy, area under receiver operating curve (AUROC) and area under precision recall curve (AUPRC). These metrics help us determine if our model is doing what we intended it for. \n","4. The above metrics are calculated based on if the labels are True positive/negative (TP/N) and False positive/negative (FP/N). Your actual data only has True positives and negatives. False predictions are done by the model. \n","5. Depending on the context of the problem we might pick which performance metric to pay attention to. For instance, if we are predicting if a person has cancer or not based on health record, it is very important that we don't miss someone with cancer (True Negative) than predicting a healthy person as cancerous (False Positive). \n","Let's define it more in numbers: Let's say we would like to predict if a person has cancer or not based on health record. And you are given 1000 data points to train with. Assume only 30 people in the data do actually have cancer. i.e. 30 TP and 970 TN, if we predict one of the 970 as having cancer it is considered False Positive (FP). We are more interested in finding people who have cancer over misclassifications and therefore we care more about the accuracy of predicting the one class over the other. \n","6. Most classification problems are set up similar to the example above. Notice we have data more on the non-cancerous people than cancerous one. This is called a class imbalance. When we have such a class imbalance it is important to pay attention to how we define the problem. \n","\n","## Let's continue with the notebook we will lean more on the way. "]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3986,"status":"ok","timestamp":1641335063453,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"uI6EXdxMtatL"},"outputs":[],"source":["#Load packages\n","import numpy as np\n","import pandas as pd\n","import os\n","import math\n","import PIL\n","from pathlib import Path\n","import glob as gb\n","from collections import Counter\n","\n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as img\n","\n","from google.colab import drive"]},{"cell_type":"markdown","metadata":{"id":"wcQnbbl7tyM6"},"source":["Lets mount the drive \n","we have already created a link to the data via shortcuts. The folder are arranged as such: \n","\n","\n","> /content/drive/MyDrive/Module0/CNN\n","\n",">> /content/drive/MyDrive/Module0/CNN/TRAIN\n","\n",">>>/content/drive/MyDrive/Module0/CNN/TRAIN/MRI\n",">>>/content/drive/MyDrive/Module0/CNN/TRAIN/PET\n","    \n",">>/content/drive/MyDrive/Module0/CNN/TEST\n","\n",">>>>/content/drive/MyDrive/Module0/CNN/TEST/MRI\n",">>>>/content/drive/MyDrive/Module0/CNN/TEST/PET\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43056,"status":"ok","timestamp":1641335106503,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"PtSa65hsvfQ6","outputId":"c3f8d817-f2ab-429c-8be0-28d282a4d7af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1641335106503,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"DOHnbeZi7Onw"},"outputs":[],"source":["train= '/content/drive/MyDrive/Module0/CNN/TRAIN/'\n","test = '/content/drive/MyDrive/Module0/CNN/TEST/'"]},{"cell_type":"markdown","metadata":{"id":"jezbYRAXImjX"},"source":["Lets check how many images there are under each label in training and validation sets"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1641335106504,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"DjTAh9u7Fe6e","outputId":"7867817e-007b-4a99-ce08-29c15370e33d"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 0 MRI images in the training folder\n","and 0 in the test folder\n","There are 0 PET images in the training folder\n","and 0 images in the test folder\n"]}],"source":["num_MRI_train_pts = len(gb.glob(train+'MRI/*.jpg'))\n","num_MRI_test_pts = len(gb.glob(test+'MRI/*.jpg'))\n","print('There are {} MRI images in the training folder'.format(num_MRI_train_pts)+'\\n'+\n","      'and {} in the test folder'.format(num_MRI_test_pts))\n","\n","\n","num_PET_train_pts = len(gb.glob(train+'PET/*.jpg'))\n","num_PET_test_pts = len(gb.glob(test+'PET/*.jpg'))\n","print('There are {} PET images in the training folder'.format(num_PET_train_pts)+'\\n'+\n","      'and {} images in the test folder'.format(num_PET_test_pts))"]},{"cell_type":"markdown","metadata":{"id":"4k3zGxDGLdID"},"source":["As you can see there is a large class imbalance, we have more MRI images than we have PET images. These images are in gray scale and have different patient data, let take a look at few of training images."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"elapsed":722,"status":"error","timestamp":1641335107222,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"mqayTf_WGNTP","outputId":"051d3f1d-68f2-4cbf-bfc7-599dd928896b"},"outputs":[{"ename":"IndexError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e8d418d6cb1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#MRI images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmri_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAHIAAABvCAYAAAAwlZQ4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABYElEQVR4nO3UwU0DUQxAwf2IEsKZ7b+WpIicoQfTQFBYKRHwNHO1D5ae5DUzG//fy28fwGMIGSFkhJARQkYIGfF6ZPl0Os2+7086hXsul8vnzLzdmh0Kue/7dj6fH3MVh621rt/NvNYIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjBAyQsgIISOEjFgz8/PltT62bbs+7xzueJ+Zt1uDQyH5u7zWCCEjhIwQMkLICCEjhIwQMkLIiC8Hohp/BhBaQAAAAABJRU5ErkJggg==","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["mri_train = gb.glob(train+'MRI/*.jpg')\n","pet_train = gb.glob(train+'PET/*.jpg')\n","\n","mri_test = gb.glob(test+'MRI/*.jpg')\n","pet_test = gb.glob(test+'PET/*.jpg')\n","\n","\n","plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    #MRI images \n","    plt.imshow(img.imread(mri_train[i]))\n","    plt.xlabel(i)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ImRk_qDfRHJS"},"source":["Lets look at the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":200,"status":"aborted","timestamp":1641335107213,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"rZIF_kSvOCnN"},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    #MRI images \n","    plt.imshow(img.imread(mri_test[i]))\n","    plt.xlabel(i)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SsHHMTJRRWe0"},"source":["Lets look at the PET image training set likewike"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":201,"status":"aborted","timestamp":1641335107214,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"fg5bzdyjRaNT"},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    #PET images \n","    plt.imshow(img.imread(pet_train[i]))\n","    plt.xlabel(i)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1JvbRZtYSLTx"},"source":["As you can see the images in the MRI data set are inclusive of cronal, axial and sagital slices. There are a few composite images and FLAIR images. Look at the PET data set they are uniformly axial slices. As we learned earlier in the lectures there are fundemental differences between what is getting imaged and how it is captured. We want the CNN to learn this.\n","\n","**Look at the PET test data on your own in the below code block**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":201,"status":"aborted","timestamp":1641335107214,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"UDkvw38ZSOPJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Xv8Pt6C2UiH-"},"source":["Next, let's load these images off disk using the helpful `tf.keras.utils.image_dataset_from_directory` utility. This will take you from a directory of images on disk to a `tf.data.Dataset` in just a couple lines of code.\n","\n","Note the image size parameter it is set to 100 X 100 means we are asking Keras to read all the images as an 100 pixel by 100 pixel resolution. This is also the dimensions of the first layer in your CNN, so if you would like for the training to run faster or slower you can tweak this parameter. But I reccomend not changing this. \n","\n","We will be splitting images in the train folder into 80% train and 20% validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":201,"status":"aborted","timestamp":1641335107214,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"OGwaAG6vUhyJ"},"outputs":[],"source":[" train_ds = tf.keras.utils.image_dataset_from_directory(\n","  train,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123, image_size=(100, 100),\n","  batch_size=32)"]},{"cell_type":"markdown","metadata":{"id":"pa5hkVEndzd3"},"source":["Similarly lets create test and validation set. This is the 20% subset of the training data. Once you finish going through this notebook. **As a homework play around with a 60/40 50/50 spilt. Notice the seed parameter it is included to improve the reproducablity of the training and validation split. If you dont specify it the code will pick a random number and everytime you run this it will give you a different result.**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":201,"status":"aborted","timestamp":1641335107215,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"5PFLxhmEewvd"},"outputs":[],"source":["valid_ds = tf.keras.utils.image_dataset_from_directory(\n","  train,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123, image_size=(100, 100),\n","  batch_size=32)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":201,"status":"aborted","timestamp":1641335107215,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"yuIz679kf7fx"},"outputs":[],"source":["# Test set will not be shown to the model \n","#it will be used to calculate performance on the trained model\n","test_ds = tf.keras.utils.image_dataset_from_directory(test,image_size=(100, 100))"]},{"cell_type":"markdown","metadata":{"id":"NoILYIYLjdr1"},"source":["These 371 images will be our independent test set which will show how our model performs on data that it hasn't seen before. "]},{"cell_type":"markdown","metadata":{"id":"y4RGdwKWilag"},"source":["Lets check the class names in the dataset, Keras directory read is designed to assign lables based on the folder names you can see how easy it is to read in the class lables and data."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":201,"status":"aborted","timestamp":1641335107215,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"fgG72SQ3dy5H"},"outputs":[],"source":["class_names = train_ds.class_names\n","print(class_names)"]},{"cell_type":"markdown","metadata":{"id":"SedlAkx3ire-"},"source":["Let's take a look at the images are in the keras dataset. Recall that we read the data into keras in batches, 32 batches to be excat. When the model is getting trained the each of these batches will be fed to the model and it should have distribution of images from both the classes. Below lets use the take function to look at a slice of the data in keras"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":202,"status":"aborted","timestamp":1641335107216,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"5nxVKQZveQQ9"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i].numpy().astype(\"uint8\"))\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"5R7-qUZ2kzIF"},"source":["\n","You will train a model using these datasets by passing them to `Model.fit` in a moment. Lets take a look at the individual batches. "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":202,"status":"aborted","timestamp":1641335107216,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"4_aSisnwenkS"},"outputs":[],"source":["for image_batch, labels_batch in train_ds:\n","  print(image_batch.shape)\n","  print(labels_batch.shape)\n","  break"]},{"cell_type":"markdown","metadata":{"id":"RwIPgEhak4tq"},"source":["The `image_batch` is a tensor of the shape `(32, 100, 100, 3)`. This is a batch of 32 images of shape `100x100x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.\n","\n","You can call `.numpy()` on the `image_batch` and `labels_batch` tensors to convert them to a `numpy.ndarray`"]},{"cell_type":"markdown","metadata":{"id":"zUaFlh5elMa6"},"source":["## Configure the dataset for performance\n","\n","Let's make sure to use buffered prefetching so you can yield data from disk without having I/O become blocking. These are two important methods you should use when loading data:\n","\n","- `Dataset.cache` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n","- `Dataset.prefetch` overlaps data preprocessing and model execution while training."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":202,"status":"aborted","timestamp":1641335107216,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"cGp8fFJhlvu7"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"v7VadSMlmLu3"},"source":["The next step is to strandardize the image. Standardization/Normalization is very important in any machine learning problem. Although our images are in gray scale they are still in RGB channel. **Refer above**\n","\n","The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network; in general you should seek to make your input values small.\n","\n","Here, you will standardize values to be in the `[0, 1]` range by using `tf.keras.layers.Rescaling`:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":203,"status":"aborted","timestamp":1641335107217,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"B3eHwEDOogdX"},"outputs":[],"source":["normalization_layer = layers.Rescaling(1./255)"]},{"cell_type":"markdown","metadata":{"id":"taXTZoRxolHM"},"source":["There are two ways to use this layer. You can apply it to the dataset by calling `Dataset.map` Or, you can include the layer inside your model definition, which can simplify deployment. Let's use the second approach here."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1641335107217,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"l3pWnDaUoktu"},"outputs":[],"source":["normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","image_batch, labels_batch = next(iter(normalized_ds))\n","first_image = image_batch[0]\n","# Notice the pixel values are now in `[0,1]`.\n","print(np.min(first_image), np.max(first_image)) "]},{"cell_type":"markdown","metadata":{"id":"_XP5IAZOq3MH"},"source":["As you can see the RGB channels are now normalized from 0 to 1, this should not change the images in any manner, just how they are represented. Lets check that"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1641335107217,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"dGwtz8jLGyjA"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","#same code block as aobve but we have changed the dataset.\n","for images, labels in normalized_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i])\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"MP5SCyCrHg1G"},"source":["Next lets move on to specifiying our model, recall the basics of convolutional neural netowrk. To gain more indepth understanding/refresher I reccomend this following video: https://www.youtube.com/watch?v=aircAruvnKk\n","\n","In simple terms we assume there is a nonlinear relationship between the input variables - pixels - and the output - lables (PET/MRI). "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1641335107218,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"VvstuOpPtgIV"},"outputs":[],"source":["num_classes = len(class_names)\n","\n","model = Sequential([\n","  layers.Rescaling(1./255, input_shape=(100, 100, 3)),\n","  layers.Conv2D(16, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(32, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(64, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Flatten(),\n","  layers.Dense(128, activation='relu'),\n","  layers.Dense(num_classes)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1641335107218,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"SPSQLKs7tp-b"},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1641335107218,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"9NXyWFDStsDg"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1641335107219,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"aMKtnkfft3VG"},"outputs":[],"source":["epochs=10\n","history = model.fit(\n","  train_ds,\n","  validation_data=valid_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1641335107219,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"WKPUsmfLuGsY"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1641335107219,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"Z8doXtXmuH73"},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","  [\n","    layers.RandomFlip(\"horizontal\",\n","                      input_shape=(100,100,3)),\n","    layers.RandomRotation(0.1),\n","    layers.RandomZoom(0.1),\n","  ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1641335107219,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"ZV8d-9vbuMDv"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","for images, _ in train_ds.take(1):\n","  for i in range(9):\n","    augmented_images = data_augmentation(images)\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1641335107220,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"uaO-Mz_RuPvx"},"outputs":[],"source":["model = Sequential([\n","  data_augmentation,\n","  layers.Rescaling(1./255),\n","  layers.Conv2D(16, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(32, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Conv2D(64, 3, padding='same', activation='relu'),\n","  layers.MaxPooling2D(),\n","  layers.Dropout(0.2),\n","  layers.Flatten(),\n","  layers.Dense(128, activation='relu'),\n","  layers.Dense(num_classes)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1641335107220,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"qlWE4zvIuUYL"},"outputs":[],"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1641335107220,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"-V_ObWTquVWI"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1641335107220,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"DW5Qzrmgubqg"},"outputs":[],"source":["epochs = 15\n","history = model.fit(\n","  train_ds,\n","  validation_data=valid_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1641335107221,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"u2Ed1LzOuf-O"},"outputs":[],"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs_range = range(epochs)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1641335107221,"user":{"displayName":"Ilmi Yoon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqsK60Bg1rPm1ufyRRcn-3XI_PH_46wltynL7muA=s64","userId":"10143487959464019796"},"user_tz":480},"id":"Kfq8iS0juhTQ"},"outputs":[],"source":["predictions = model.predict(test_ds)\n","score = tf.nn.softmax(predictions[0])\n","\n","print(\n","    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n","    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Module0_NB2_v0.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
